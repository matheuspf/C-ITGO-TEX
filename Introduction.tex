\section{Introduction} \label{sec:Introduction}

In recent years, there has been an increase in the interest of applying meta-heuristics to solve difficult numerical optimization problems, mainly due to the impossibility of obtaining good quality solutions to some complex problems using deterministic methods. It is usual to find nonlinear constrained real-world problems which are not smooth or not even continuous, ruling out the possibility to apply any gradient-based optimization procedure.

Even when the problem at hand has continuous derivatives, it may be multimodal, having many local optimum points. Any deterministic procedure is confined to finding sub-optimal solutions for any multimodal problem of reasonable size. Many modern meta-heuristics, on the other hand, are capable of finding an optimal or nearly optimal solution to these problems within a feasible amount of time.

Among the most recent meta-heuristics used to solve nonlinear constrained optimization problems, we can cite some well known methods, such as Particle Swarm Optimization (PSO) \citep{IPSO, IAPSO, PSO1}, Artificial Bee Colony (ABC) \citep{CB-ABC, IABC-Mal}, Differential Evolution (DE) \citep{DE1, DE2, MVDE}, Genetic Algorithms (GA) \citep{GA1} and many new methods, some being inspired in nature \citep{CS, WCA, MBA}.

The objective is to find the best solution in a search space that satisfies some criteria, expressed in the form of constraints, whose value is rated according to a function. For minimization, a general nonlinearly constrained optimization problem can be defined as follows: \\[-3em]

\begin{equation}\label{eq:func}
    \begin{aligned}
    \underset{\bm{x}}{\text{\fnt{10} arg\,min}} \ \ & f(\bm{x}) \\
    \text{s.t.} \quad & g_j(\bm{x}) \leq 0, \qquad j = 1, ..., p  \\
                      & h_k(\bm{x}) = 0, \qquad k = 1, ..., q  \\
                      & l_i \leq x_i \leq u_i, \qquad i = 1, ..., n 
    \end{aligned}
\end{equation}

\noindent
where $f(\cdot)$ is the objective function or fitness function, $\bm{x} = (x_1, ..., x_n)$ is a solution vector containing $n$ continuous or discrete values, $g_j(\cdot), \ j = 1, ..., p$, and $h_k(\cdot), \ k = 1, ..., q$, are the inequality and equality constraints respectively, where both can be linear or nonlinear. The vectors $\bm{l}$ and $\bm{u}$ define the lower and upper bounds of the solution space. We also define the sum of constraint violation as: \\[-3em]

\begin{equation}\label{eq:viol}
    v(\bm{x}) = \sum_j^p max(g_j(\bm{x}), 0) + \sum_k^q |h_k(\bm{x})|
\end{equation}

\noindent
so a solution $\bm{x}$ is feasible only when $v(\bm{x}) = 0$. 

In this work, we develop some modifications over a successful meta-heuristic for continuous optimization, know as Iterative Topographical Global Optimization. The modifications consist of adding skills to ITGO to work with constrained optimization issues, calling this new algorithm as Contrained version of I-TGO (CI-TGO). We compare the results obtained on eight complex constrained engineering design problems used as benchmark against several state of the art methods, as the C-ITGO outperforming the best competing techniques of literature, mainly relative to the number of function evaluations (NFEs) that characterize the execution effort of the technique.

The structure of the paper is as follows. Section \ref{sec:Methods} presents the CI-TGO method, the details of its implementation and the parameters used in the tests. We compared the developed algorithm against the competing methods in Section \ref{sec:Results}, and make the final conclusions in Section \ref{sec:Conclusion}.